[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A brief introduction to Python for geocomputation",
    "section": "",
    "text": "1 Overview\nThis is an introduction to the world of geospatial analysis, or geocomputation, using Python. Since you are in this class, you already have a good working knowledge of using Python for data analytics work. I’m sure you all have a basic familiarity with maps through using things like Google Maps, actual paper maps and a globe. We’ll learn the basic geospatial concepts needed such as coordinate reference systems and the various types of data used in this realm."
  },
  {
    "objectID": "index.html#what-is-this-module-all-about",
    "href": "index.html#what-is-this-module-all-about",
    "title": "A brief introduction to Python for geocomputation",
    "section": "1.1 What is this module all about?",
    "text": "1.1 What is this module all about?\nThis is an introduction to the world of geospatial analysis, or geocomputation, using Python. Since you are in this class, you already have a good working knowledge of using Python for data analytics work. I’m sure you all have a basic familiarity with maps through using things like Google Maps, actual paper maps and a globe. We’ll learn the basic geospatial concepts needed such as coordinate reference systems and the various types of data used in this realm."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "A brief introduction to Python for geocomputation",
    "section": "1.1 Acknowledgements",
    "text": "1.1 Acknowledgements\nI have adapted the materials from the Introduction to Geospatial Raster and Vector Data with Python course, which is part of the Carpentries Incubator. The Carpentries Incubator is a hub for people creating open access course materials using the templates and other resources provided by The Carpentries. The goal of The Carpentries is:\n\nWe teach foundational coding and data science skills to researchers worldwide.\n\nI have used materials from Software Carpentry in my Practical Computing for Data Analytics course.\nAll of the materials I’m using are licensed under CC-BY 4.0. Similarly, all of my materials in this module use the same CC-BY 4.0 license.\nMy original plan was to use the templates from The Carpentries for this module as they are based on using R Markdown. However, recently they lost funding that led to layoffs and it doesn’t look like there are resoures for continued development of the Carpentries Workbench. With the advent of Quarto, I was hoping that the Carpentries templates would evolve to support it, but that doesn’t seem to be in the cards. So, I’m using some of their content and ideas, but am developing this module using Quarto. Quarto is a computational authoring tool that supports multiple programming languages. While Quarto is the next generation of R Markdown, it also supports Python code chunks. You can find the GitHub repo for this module at https://github.com/misken/geonewb_python.\nThis module will also use the extremely good online texts:\n\nGeocomputation with Python (GCwP)\nGeocomputation with R (GCwR)"
  },
  {
    "objectID": "index.html#module-structure",
    "href": "index.html#module-structure",
    "title": "A brief introduction to Python for geocomputation",
    "section": "1.2 Module structure",
    "text": "1.2 Module structure\nThere are a series of sections, or chapters, which cover a range of topics. Within each section, there will also be a Jupyter notebook that you will work through to get hands-on practice with each topic. The series of notebooks are based on an overarching case study involving analysis of land use on the Oakland University campus.\nHere’s an overview of the topics we’ll cover in this module.\n\nIntroduction to Raster Data\n\nwhat is it and what is it used to represent?\nmain Python libraries for working with raster data\nbasic plotting of raster data\nCase Study: reading, exploring, plotting a raster file representing land use on the OU campus\n\nIntroduction to Vector Data\n\nwhat is it and what is it used to represent?\nmain Python libraries for working with vector data\nbasic plotting of vector data\nCase Study: reading, exploring, plotting a vector file representing various features on the OU campus such as buildings and rivers\n\nCoordinate Reference Systems\n\nthis is a big, important, and complex topic\nhow do we represent something like location on our planet, that is inherently 3D, on a 2D surface such as a computer screen or a piece of paper? We use something called a projected coordinate reference system (CRS).\nwe will learn enough to be able to work effectively with data from different coordinate reference systems.\nCase Study: check CRS of raster and vector data, reproject data from one CRS to another\n\nThe GIS Landscape\n\nit’s a big landscape\nsoftware options\n\nAccessing Satellite Imagery\n\nuse MS Planetary Computer to find and download satellite imagery\nCase Study: Download and view Sentinel-2 images of OU campus\n\nWorking with Raster and Vector Data in Python\n\nusing rioxarray and xarray packages to read, explore, and visualize raster data\nmore use of GeoPandas for vector data manipulation\ncommon raster operations such as cropping, dealing with no data values, reprojecting and resampling\nraster computations such as NVDI\nCase Study: analyze land use patterns\n\n\n\n1.2.1 Software setup for the case study\nYou should already have the Anaconda distribution of Python installed and working. Due to the large number of libraries needed for geospatial analysis, we are also going to create and use a new Conda virtual environment for this module.\nStep 1 - Download the geonewb.yml conda environment YAML file.\nStep 2 - Open an Anaconda Prompt and change directories to the place where you downloaded geonewb.yml. Create the new environment with:\nconda env create -f geonewb.yml\n\n\n\n\n\n\nWarning\n\n\n\nThis can take several minutes. Be patient.\n\n\nLater, when you are working in Jupyter notebooks for this module, you’ll select the geonewb virtual environment to use for your work."
  },
  {
    "objectID": "01_intro_raster_data.html#readings-and-resources",
    "href": "01_intro_raster_data.html#readings-and-resources",
    "title": "2  Introduction to raster data",
    "section": "2.1 Readings and resources",
    "text": "2.1 Readings and resources\n\nGCwP - Ch 1: Geographic Data in Python - Section 1.3\nGCwR - Ch 2: Geographic Data in R - Section 2.3\nGIS Stack Exchange is like StackOverflow for geospatial questions. Very helpful.\n\nYes, I know this class is Python based, but I’m including the second reference for a few reasons. First of all, it has outstanding, well written, content. Secondly, R has many strengths for geospatial analysis and mapping and most of you have already learned both R and Python basics in my previous course. It’s good to be aware of the R ecosystem for geocomputation as then you can pick and choose your tools as you see fit. R and Python are becoming more interoperable as time goes on and mixing the two is becoming quite common."
  },
  {
    "objectID": "01_intro_raster_data.html#data-structures-raster-and-vector",
    "href": "01_intro_raster_data.html#data-structures-raster-and-vector",
    "title": "2  Introduction to raster data",
    "section": "2.2 Data structures: raster and vector",
    "text": "2.2 Data structures: raster and vector\nThe two primary types of geospatial data are raster and vector data. Raster data is stored as a grid of values which are rendered on a map as pixels. Each pixel value represents an area on the Earth’s surface. Vector data structures represent specific features on the Earth’s surface (e.g. boundaries, bodies of water, roads, buildings, dams, parks, …), and assign attributes to those features. Vector data structures will be discussed in more detail in Chapter 3.\nThis module will focus on how to work with both raster and vector data sets, therefore it is essential that we understand the basic structures of these types of data and the types of data that they can be used to represent. Many maps have a combination of vector and raster data. For example, when you use satellite view in Google Maps, you are seeing vector data such as roads and places layered on top of aerial imagery which is raster data."
  },
  {
    "objectID": "01_intro_raster_data.html#about-raster-data",
    "href": "01_intro_raster_data.html#about-raster-data",
    "title": "2  Introduction to raster data",
    "section": "2.3 About raster data",
    "text": "2.3 About raster data\nRaster data is any pixelated (or gridded) data where each pixel is associated with a specific geographic location. The value of a pixel can be continuous (e.g. elevation) or categorical (e.g. land use). If this sounds familiar, it is because this data structure is very common: it’s how we represent any digital image. A geospatial raster is only different from a digital photo in that it is accompanied by spatial information that connects the data to a particular location. This includes the raster’s extent (the area represented by the raster), cell size, the number of rows and columns, and its coordinate reference system (or CRS).\n\n\n\nRaster Concept (Source: National Ecological Observatory Network (NEON))\n\n\nSome examples of continuous rasters include:\n\nPrecipitation maps,\nMaps of tree height derived from lidar data,\nElevation values for a region.\n\nA map of elevation for Harvard Forest derived from the NEON AOP LiDAR sensor is below. Elevation is represented as a continuous numeric variable in this map. The legend shows the continuous range of values in the data from around 300 to 420 meters.\n\n\n\nContinuous Elevation Map: HARV Field Site\n\n\nSome rasters contain categorical data where each pixel represents a discrete class such as a landcover type (e.g., “forest” or “grassland”) rather than a continuous value such as elevation or temperature. Some examples of classified maps include:\n\nLandcover / land-use maps,\nTree height maps classified as short, medium, and tall trees.\nElevation maps classified as low, medium, and high elevation.\n\n\n\n\nUSA landcover classification\n\n\nThe map above shows the contiguous United States with landcover as categorical data. Each color is a different landcover category. (Source: Homer, C.G., et al., 2015, Completion of the 2011 National Land Cover Database for the conterminous United States-Representing a decade of land cover change information. Photogrammetric Engineering and Remote Sensing, v. 81, no. 5, p. 345-354)\n\n\n\n\n\n\nChallenge - Advantages and Disadvantages\n\n\n\nThink about potential advantages and disadvantages of storing data in raster format and list them.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRaster data has some important advantages:\n\nrepresentation of continuous surfaces\npotentially very high levels of detail\ndata is ‘unweighted’ across its extent - the geometry doesn’t implicitly highlight features\ncell-by-cell calculations can be very fast and efficient\n\nThe downsides of raster data are:\n\nvery large file sizes as cell size gets smaller\ncurrently popular formats don’t embed metadata well (more on this later!)\ncan be difficult to represent complex information"
  },
  {
    "objectID": "01_intro_raster_data.html#important-attributes-of-raster-data",
    "href": "01_intro_raster_data.html#important-attributes-of-raster-data",
    "title": "2  Introduction to raster data",
    "section": "2.4 Important attributes of raster data",
    "text": "2.4 Important attributes of raster data\n\n2.4.1 Resolution\nA resolution of a raster represents the area on the ground that each pixel of the raster covers. The image below illustrates the effect of changes in resolution.\n\n\n\nResolution image (Source: National Ecological Observatory Network (NEON))"
  },
  {
    "objectID": "01_intro_raster_data.html#raster-data-format-for-this-module",
    "href": "01_intro_raster_data.html#raster-data-format-for-this-module",
    "title": "2  Introduction to raster data",
    "section": "2.5 Raster data format for this module",
    "text": "2.5 Raster data format for this module\nRaster data can come in many different formats. We will use the GeoTIFF format which has the extension .tif. A .tif file stores metadata or attributes about the file as embedded tif tags along with the actual raster data. For instance, your camera might store a tag that describes the make and model of the camera or the date the photo was taken when it saves a .tif. A GeoTIFF is a standard .tif image format with additional spatial (georeferencing) information embedded in the file as tags. These tags should include the following raster metadata:\n\nExtent\nResolution\nCoordinate Reference System (CRS) - we will introduce this concept in Chapter 4.\nValues that represent missing data (NoDataValue)\n\nWe will discuss these attributes in more detail in later sections.\n\n\n\n\n\n\nMore Resources on the .tif format\n\n\n\n\nGeoTIFF on Wikipedia\nOSGEO TIFF documentation"
  },
  {
    "objectID": "01_intro_raster_data.html#multi-band-raster-data",
    "href": "01_intro_raster_data.html#multi-band-raster-data",
    "title": "2  Introduction to raster data",
    "section": "2.6 Multi-band raster data",
    "text": "2.6 Multi-band raster data\nA raster can contain one or more bands. One type of multi-band raster dataset that is familiar to many of us is a color image. A basic color image consists of three bands: red, green, and blue. Each band represents light reflected from the red, green or blue portions of the electromagnetic spectrum. The pixel brightness for each band, when composited creates the colors that we see in an image.\n\n\n\nRGB multi-band raster image (Source: National Ecological Observatory Network (NEON).)\n\n\nWe can plot each band of a multi-band image individually. Or, we can composite all three bands together to make a color image. In a multi-band dataset, the individual band rasters will always have the same extent, resolution, and CRS.\nMulti-band raster data might also contain:\n\nTime series: the same variable, over the same area, over time.\nMulti or hyperspectral imagery: image rasters that have 4 or more (multi-spectral) or more than 10-15 (hyperspectral) bands. We won’t be working with this type of data in this module, but you can check out the NEON Data Skills Imaging Spectroscopy HDF5 in R tutorial if you’re interested in working with hyperspectral data cubes.\n\n\n\n\n\n\n\nNote\n\n\n\n\nRaster data is pixelated data where each pixel is associated with a specific location.\nRaster data always has an extent and a resolution.\nThe extent is the geographical area covered by a raster.\nThe resolution is the area covered by each pixel of a raster."
  },
  {
    "objectID": "01_intro_raster_data.html#case-study-land-use-analysis-on-the-ou-campus",
    "href": "01_intro_raster_data.html#case-study-land-use-analysis-on-the-ou-campus",
    "title": "2  Introduction to raster data",
    "section": "2.7 Case Study: Land use analysis on the OU campus",
    "text": "2.7 Case Study: Land use analysis on the OU campus\nThroughout this module, we will use this case study to allow you to get some hands on practice with the topics covered. For this first module, you’ll be working through a Jupyter notebook that introduces the overall case study and gives you an introduction to working with relevant raster data representing land use on the OU campus. You’ll get your first look at two import Python packages for working with raster data - xarray and rioxarray.\n\n2.7.1 xarray - labelled multidimension arrays\nXarray builds on top of NumPy N-d arrays and adds the ability to create and work with labels for the dimensions.\n\nXarray makes working with labelled multi-dimensional arrays in Python simple, efficient, and fun!\n\nThe two main data structures are DataArray (a N-d generalization of a pandas.Series) and DataSet (an N-d generalization of a pandas.DataFrame). The Overview: Why xarray? page has a nice level of detail on the case for xarray and its link to geospatial analysis.\n\n\n2.7.2 rioxarray - read raster data into xarray objects\nThe rioxarray package extends the xarray package to facilitate reading raster data into xarray objects. The actual reading of the raster file is done using another Python package known as rasterio. Once you load rioxarray, your can use the rio accessor with xarray objects to access rioxarray methods for working with raster data. From the rasterio docs:\n\nGeographic information systems use GeoTIFF and other formats to organize and store gridded raster datasets such as satellite imagery and terrain models. Rasterio reads and writes these formats and provides a Python API based on Numpy N-dimensional arrays and GeoJSON.\n\n\n\n2.7.3 Activities\nI have created one compressed archive which contains the Jupyter notebook(s) as well as any data (or other) files needed.\nStart by downloading the following file and extract it in a location of your choice.\nLaunch Jupyter lab and open the ou_land_use_01_rasterintro.ipynb file. Work your way through it.\n\nDownload: ou_land_use_analysis.zip"
  },
  {
    "objectID": "02_intro_vector_data.html#readings-and-resources",
    "href": "02_intro_vector_data.html#readings-and-resources",
    "title": "3  Introduction to vector data",
    "section": "3.1 Readings and resources",
    "text": "3.1 Readings and resources\n\nGCwP - Ch 1: Geographic Data in Python - Section 1.2\nGCwR - Ch 2: Geographic Data in R - Section 2.2"
  },
  {
    "objectID": "02_intro_vector_data.html#about-vector-data",
    "href": "02_intro_vector_data.html#about-vector-data",
    "title": "3  Introduction to vector data",
    "section": "3.2 About vector data",
    "text": "3.2 About vector data\nVector data structures represent specific features on the Earth’s surface, and assign attributes to those features. Vectors are composed of discrete geometric locations (x, y values) known as vertices that define the shape of the spatial object. The organization of the vertices determines the type of vector that we are working with: point, line or polygon.\n\n\n\nvector data types\n\n\n\nPoints: Each point is defined by a single x, y coordinate. There can be many points in a vector point file. Examples of point data include: sampling locations, the location of individual buildings, or the location of bathrooms.\nLines: Lines are composed of many (at least 2) points that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each “bend” in the road or stream represents a vertex that has a defined x, y location.\nPolygons: A polygon consists of 3 or more vertices that are connected and closed. The outlines of survey plot boundaries, lakes, oceans, and states or countries are often represented by polygons.\n\n\n\n\n\n\n\nData Tip\n\n\n\nSometimes, boundary layers such as states and countries, are stored as lines rather than polygons. However, these boundaries, when represented as a line, will not create a closed object with a defined area that can be filled.\n\n\n\n\n\n\n\n\nChallenge - Identify Vector Types\n\n\n\nThe plot below includes examples of two of the three types of vector objects. Use the definitions above to identify which features are represented by which vector type.\n\n\n\nvector type examples\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nState boundaries are polygons. The Fisher Tower location is a point. There are no line features shown.\n\n\n\nVector data has some important advantages:\n\nThe geometry itself contains information about what the dataset creator thought was important.\nThe geometry structures hold information in themselves - why choose point over polygon, for instance?\nEach geometry feature can carry multiple attributes instead of just one, e.g. a database of cities can have attributes for name, country, population, etc.\nData storage can be very efficient compared to rasters. For example, a polygon of a large area can be represented with a small number of vertices as compared to all the raster grid elements making up the polygon.\n\nThe downsides of vector data include:\n\nPotential loss of detail compared to raster.\nPotential bias in datasets - what didn’t get recorded?\nCalculations involving multiple vector layers need to do math on the geometry as well as the attributes, so can be slow compared to raster math.\n\nVector datasets are in use in many industries besides geospatial fields. For instance, computer graphics are largely vector-based, although the data structures in use tend to join points using arcs and complex curves rather than straight lines. Computer-aided design (CAD) is also vector-based. The difference is that geospatial datasets are accompanied by information tying their features to real-world locations."
  },
  {
    "objectID": "02_intro_vector_data.html#vector-data-format-for-this-module",
    "href": "02_intro_vector_data.html#vector-data-format-for-this-module",
    "title": "3  Introduction to vector data",
    "section": "3.4 Vector data format for this module",
    "text": "3.4 Vector data format for this module\nLike raster data, vector data can also come in many different formats. For this module, we will start with the Shapefile format. This is an extremely common (and old) format developed by ESRI the company behind the most popular commercial GIS package, ArcGIS.\nA Shapefile format consists of multiple files in the same directory, of which .shp, .shx, and .dbf files are mandatory. Other non-mandatory but very important files are .prj and shp.xml files.\n\nThe .shp file stores the feature geometry itself\n.shx is a positional index of the feature geometry to allow quickly searching forwards and backwards through the geographic coordinates of each vertex in the vector\n.dbf contains the tabular attributes for each shape. Based on the really old (1983, pre-Windows), dBASE, file format.\n.prj file indicates the coordinate reference system (CRS)\n.shp.xml contains the Shapefile metadata.\n\nTogether, the Shapefile includes the following information:\n\nExtent - the spatial extent of the shapefile (i.e. geographic area that the shapefile covers). The spatial extent for a shapefile represents the combined extent for all spatial objects in the shapefile.\nObject type - whether the shapefile includes points, lines, or polygons.\nCoordinate reference system (CRS) - more on this later.\nOther attributes - for example, a line shapefile that contains the locations of streams, might contain the name of each stream.\n\nBecause the structure of points, lines, and polygons are different, each individual shapefile can only contain one vector type (all points, all lines or all polygons). You will not find a mixture of point, line and polygon objects in a single shapefile.\nLater in this module we’ll look at GeoJSON files, another format for storing vector data.\n\n\n\n\n\n\nMore resources on shapefiles\n\n\n\nMore about shapefiles can be found on Wikipedia. Shapefiles are often publicly available from government services, such as this page from the US Census Bureau."
  },
  {
    "objectID": "02_intro_vector_data.html#integrated-raster-and-vector-data-formats",
    "href": "02_intro_vector_data.html#integrated-raster-and-vector-data-formats",
    "title": "3  Introduction to vector data",
    "section": "3.5 Integrated raster and vector data formats",
    "text": "3.5 Integrated raster and vector data formats\nVery few formats can contain both raster and vector data - in fact, most are even more restrictive than that. Vector datasets are usually locked to one geometry type, e.g. points only. Raster datasets can usually only encode one data type, for example you can’t have a multiband GeoTIFF where one layer is integer data and another is floating-point. There are sound reasons for this - format standards are easier to define and maintain, and so is metadata. The effects of particular data manipulations are more predictable if you are confident that all of your input data has the same characteristics.\nThere are integrated file formats that do allow you to mix separate vector and raster files within the same container file. These include GeoPackage (which is a SQLite database) and Geodatabase formats.\n\n\n\n\n\n\n\nVector data structures represent specific features on the Earth’s surface along with attributes of those features.\nVector objects are either points, lines, or polygons.\n\n\n\n\nHow might we store and use vector data in Python?"
  },
  {
    "objectID": "02_intro_vector_data.html#add-geospatial-functionality-to-pandas",
    "href": "02_intro_vector_data.html#add-geospatial-functionality-to-pandas",
    "title": "3  Introduction to vector data",
    "section": "3.6 Add geospatial functionality to pandas",
    "text": "3.6 Add geospatial functionality to pandas\nThe basic idea behind GeoPandas is to combine the capabilities of pandas with the shapely library to allow you to work with geospatial data in a pandas-like way.\n\nGeoPandas is an open source project to make working with geospatial data in python easier. GeoPandas extends the datatypes used by pandas to allow spatial operations on geometric types. Geometric operations are performed by shapely. GeoPandas further depends on fiona for file access and matplotlib for plotting.\n\n\nGeoPandas extends the popular pandas library for data analysis to geospatial applications. The main pandas objects (the Series and the DataFrame) are expanded to geopandas objects (GeoSeries and GeoDataFrame). This extension is implemented by including geometric types, represented in Python using the shapely library, and by providing dedicated methods for spatial operations (union, intersection, etc.). The relationship between Series, DataFrame, GeoSeries and GeoDataFrame can be briefly explained as follow:\n\nA Series is a one-dimensional array with axis, holding any data type (integers, strings, floating-point numbers, Python objects, etc.)\nA DataFrame is a two-dimensional labeled data structure with columns of potentially different types1.\nA GeoSeries is a Series object designed to store shapely geometry objects.\nA GeoDataFrame is an extended pandas.DataFrame, which has a column with geometry objects, and this column is a GeoSeries.\n\nLet’s use geopandas to read a shapefile. The US government provides Cartographic Boundary Files in both geodatabase and shapefile formats. They are available at different levels of resolution and also by geographic region.\n\nThe cartographic boundary files are simplified representations of selected geographic areas from the Census Bureau’s Master Address File/Topologically Integrated Geographic Encoding and Referencing (MAF/TIGER) System. These boundary files are specifically designed for small scale thematic mapping. As of 2019, cartographic boundary files are available in shapefile, geodatabase, and Keyhole Markup Language (KML) format. For more details about these files, including their appropriate usage, please see our Cartographic Boundary File Description page.\n\nFrom this link, I downloaded a zipped shapefile (cb_2022_26_place_500k.zip) for the Places in the state of Michigan. As we’ll see, these are things like city and township boundaries. After uncompressing it, the resulting folder looks like this:\n├── cb_2022_26_place_500k\n│   ├── cb_2022_26_place_500k.cpg\n│   ├── cb_2022_26_place_500k.dbf\n│   ├── cb_2022_26_place_500k.prj\n│   ├── cb_2022_26_place_500k.shp\n│   ├── cb_2022_26_place_500k.shp.ea.iso.xml\n│   ├── cb_2022_26_place_500k.shp.iso.xml\n│   └── cb_2022_26_place_500k.shx\n└── cb_2022_26_place_500k.zip\n\n1 directory, 8 files\nNow let’s open the shapefile with geopandas.\n\nimport geopandas as gpd\nfrom pathlib import Path\n\nTo read a shapefile, we can use the read_file() function and pass in the name of the file with the .shp extension.\n\nmi_places_file = Path('data', 'cb_2022_26_place_500k', 'cb_2022_26_place_500k.shp')\nmi_places_gdf = gpd.read_file(mi_places_file)\n\nmi_places_gdf.info()\n\n<class 'geopandas.geodataframe.GeoDataFrame'>\nRangeIndex: 745 entries, 0 to 744\nData columns (total 13 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   STATEFP     745 non-null    object  \n 1   PLACEFP     745 non-null    object  \n 2   PLACENS     745 non-null    object  \n 3   AFFGEOID    745 non-null    object  \n 4   GEOID       745 non-null    object  \n 5   NAME        745 non-null    object  \n 6   NAMELSAD    745 non-null    object  \n 7   STUSPS      745 non-null    object  \n 8   STATE_NAME  745 non-null    object  \n 9   LSAD        745 non-null    object  \n 10  ALAND       745 non-null    int64   \n 11  AWATER      745 non-null    int64   \n 12  geometry    745 non-null    geometry\ndtypes: geometry(1), int64(2), object(10)\nmemory usage: 75.8+ KB\n\n\nThe resulting data structure is called a GeoDataFrame. In addition to standard DataFrame fields, a GeoDataFrame has one or more geometry fields. Only one geometry field is considered active at any one time. As you might guess, this field will allow us to do spatial queries with pandas-like commands as well as plot the data.\nLet’s see what’s in the geometry field.\n\nmi_places_gdf[['NAME', 'geometry']].head(15)\n\n\n\n\n\n  \n    \n      \n      NAME\n      geometry\n    \n  \n  \n    \n      0\n      Wakefield\n      POLYGON ((-89.98990 46.47580, -89.98972 46.477...\n    \n    \n      1\n      Hesperia\n      POLYGON ((-86.04946 43.57584, -86.03964 43.575...\n    \n    \n      2\n      Britton\n      POLYGON ((-83.84044 41.98837, -83.83784 41.987...\n    \n    \n      3\n      Zilwaukee\n      POLYGON ((-83.93590 43.47955, -83.93557 43.494...\n    \n    \n      4\n      Zeeland\n      POLYGON ((-86.03850 42.80958, -86.03564 42.812...\n    \n    \n      5\n      Sunfield\n      POLYGON ((-85.00503 42.76676, -84.98533 42.766...\n    \n    \n      6\n      Suttons Bay\n      POLYGON ((-85.65931 44.98558, -85.65684 44.985...\n    \n    \n      7\n      Big Rapids\n      POLYGON ((-85.50392 43.68804, -85.50381 43.695...\n    \n    \n      8\n      Fife Lake\n      POLYGON ((-85.36328 44.57706, -85.36324 44.580...\n    \n    \n      9\n      Bloomfield Hills\n      POLYGON ((-83.26587 42.59630, -83.25753 42.596...\n    \n    \n      10\n      Tustin\n      POLYGON ((-85.46393 44.10665, -85.45380 44.106...\n    \n    \n      11\n      Melvin\n      POLYGON ((-82.87187 43.19339, -82.85227 43.193...\n    \n    \n      12\n      Roosevelt Park\n      POLYGON ((-86.28329 43.20215, -86.28320 43.205...\n    \n    \n      13\n      Hudson\n      POLYGON ((-84.36191 41.86440, -84.36097 41.863...\n    \n    \n      14\n      North Branch\n      POLYGON ((-83.21524 43.22503, -83.20216 43.225...\n    \n  \n\n\n\n\nAs each row is a “place” in Michigan, it’s not surprising to find POLYGON objects in the geometry field. You’ll explore this GeoDataFrame further in the land use analysis notebook below."
  },
  {
    "objectID": "02_intro_vector_data.html#case-study-land-use-analysis-on-the-ou-campus",
    "href": "02_intro_vector_data.html#case-study-land-use-analysis-on-the-ou-campus",
    "title": "3  Introduction to vector data",
    "section": "3.7 Case Study: Land use analysis on the OU campus",
    "text": "3.7 Case Study: Land use analysis on the OU campus\nFor this this module, you’ll be working through a Jupyter notebook that introduces the very basics of working with vector data in shapefile format.\n\n3.7.1 Activities\nLaunch Jupyter lab and open the ou_land_use_02_vectorintro.ipynb file. Work your way through it."
  },
  {
    "objectID": "03_intro_crs.html#readings-and-resources",
    "href": "03_intro_crs.html#readings-and-resources",
    "title": "4  Introduction to coordinate reference systems",
    "section": "4.1 Readings and resources",
    "text": "4.1 Readings and resources\n\nGCwP - Ch 1: Geographic Data in Python - Section 1.4\nGCwR - Section 2.4: Coordinate Reference Systems\nbooklet written by ESRI"
  },
  {
    "objectID": "03_intro_crs.html#coordinate-reference-systems",
    "href": "03_intro_crs.html#coordinate-reference-systems",
    "title": "4  Introduction to coordinate reference systems",
    "section": "4.2 Coordinate reference systems",
    "text": "4.2 Coordinate reference systems\nA data structure cannot be considered geospatial unless it is accompanied by coordinate reference system (CRS) information, in a format that geospatial applications can use to display and manipulate the data correctly. CRS information connects data to the Earth’s surface using a mathematical model.\nOne very common CRS that you will encounter is known as EPSG:4326 (more on names later). EPSG:4326 is a geographic coordinate system and not a projected coordinate system.\nA good explanation of the difference between these two things is provided by ESRI.\n\n\nA GCS defines where the data is located on the earth’s surface.\nA PCS tells the data how to draw on a flat surface, like on a paper map or a computer screen.\n\n\nand\n\nA projected coordinate system (PCS) is a GCS that has been flattened using a map projection.\n\nThe EPSG:4326 CRS uses longitude and latitude to locate points on an underlying model of the earth (WGS84). But, this is a 3D model and any attempt to represent it in 2D (via one of countless possible projections) will lead to different degrees of distortion in area, distance and angle conformance. Before plotting geographic data on some 2D surface (e.g. screen or paper) we need to pick an appropriate projected coordinate reference system.\nThe CRS associated with a dataset tells your mapping software (for example Python) where the data is located in geographic space. It also tells the mapping software what method should be used to flatten or project the data in geographic space.\n\n\n\nMaps of the United States in different projections (Source: opennews.org)\n\n\nThe above image shows maps of the United States in different projections. Notice the differences in shape associated with each projection. These differences are a direct result of the calculations used to flatten the data onto a 2-dimensional map.\nThere are lots of great resources that describe coordinate reference systems and projections in greater detail. For the purposes of this module, what is important to understand is that data from the same location but saved in different projections will not line up in any GIS or other program. Thus, it’s important when working with spatial data to identify the coordinate reference system applied to the data and retain it throughout data processing and analysis."
  },
  {
    "objectID": "03_intro_crs.html#components-of-a-crs",
    "href": "03_intro_crs.html#components-of-a-crs",
    "title": "4  Introduction to coordinate reference systems",
    "section": "4.3 Components of a CRS",
    "text": "4.3 Components of a CRS\nCRS information has three components:\n\nDatum: A model of the shape of the earth. It has angular units (degrees) and defines the starting point (where is [0,0]?) so that the angles reference a meaningful spot on the earth. Common global datums are WGS84 and NAD83. Datums can also be local - fit to a particular area of the globe, but ill-fitting outside the area of intended use. In this module, we will use the WGS84 datum.\nProjection: A mathematical transformation of the angular measurements on a round earth to a flat surface (i.e. paper or a computer screen). The units associated with a given projection are usually linear (feet, meters, etc.). In this module, we will see data in a few different projections.\nAdditional Parameters: Additional parameters are often necessary to create the full coordinate reference system. One common additional parameter is a definition of the center of the map. The number of required additional parameters depends on what is needed by each specific projection.\n\n\n\n\n\n\n\nEPSG:4326 and WGS84\n\n\n\nSometimes it seems like EPSG:4326 and WGS84 are used synonymously. The folks at the MapScaping podcast have done a nice blog post on the relationship between EPSG:4326 and WGS84."
  },
  {
    "objectID": "03_intro_crs.html#orange-peel-analogy",
    "href": "03_intro_crs.html#orange-peel-analogy",
    "title": "4  Introduction to coordinate reference systems",
    "section": "4.4 Orange peel analogy",
    "text": "4.4 Orange peel analogy\nA common analogy employed to teach projections is the orange peel analogy. If you imagine that the Earth is an orange, how you peel it and then flatten the peel is similar to how projections get made.\n\nA datum is the choice of fruit to use. Is the Earth an orange, a lemon, a lime, a grapefruit?\n\n\n\n\nDatum Fruit Example (Image source)\n\n\nA projection is how you peel your orange and then flatten the peel.\n\n\n\nProjection Citrus Peel Example (Image from Prof Drika Geografia, Projeções Cartográficas)\n\n\n\nAn additional parameter could include a definition of the location of the stem of the fruit. What other parameters could be included in this analogy?"
  },
  {
    "objectID": "03_intro_crs.html#which-projection-should-i-use",
    "href": "03_intro_crs.html#which-projection-should-i-use",
    "title": "4  Introduction to coordinate reference systems",
    "section": "4.5 Which projection should I use?",
    "text": "4.5 Which projection should I use?\nTo decide if a projection is right for your data, answer these questions:\n\nWhat is the area of minimal distortion?\nWhat aspect of the data does it preserve?\n\nPeter Dana from the University of Colorado at Boulder and the Department of Geo-Information Processing have a good discussion of these aspects of projections. Online tools like Projection Wizard can also help you discover projections that might be a good fit for your data.\n\n\n\n\n\n\nData Tip\n\n\n\nTake the time to identify a projection that is suited for your project. You don’t have to stick to the ones that are popular."
  },
  {
    "objectID": "03_intro_crs.html#describing-coordinate-reference-systems",
    "href": "03_intro_crs.html#describing-coordinate-reference-systems",
    "title": "4  Introduction to coordinate reference systems",
    "section": "4.6 Describing coordinate reference systems",
    "text": "4.6 Describing coordinate reference systems\nThere are several common systems in use for storing and transmitting CRS information, as well as translating among different CRSs. These systems generally comply with ISO 19111. Common systems for describing CRSs include EPSG, OGC WKT, and PROJ strings.\n\n4.6.1 EPSG\nThe EPSG system is a database of CRS information maintained by the International Association of Oil and Gas Producers. The dataset contains both CRS definitions and information on how to safely convert data from one CRS to another. Using EPSG is easy as every CRS has an integer identifier, e.g. WGS84 is EPSG:4326. The downside is that you can only use the CRSs defined by EPSG and cannot customise them (some datasets do not have EPSG codes). epsg.io is an excellent website for finding suitable projections by location or for finding information about a particular EPSG code.\n\n\n4.6.2 Well-Known text (WKT)\nThe Open Geospatial Consortium WKT standard is used by a number of important geospatial apps and software libraries. WKT is a nested list of geodetic parameters. The structure of the information is defined on their website. WKT is valuable in that the CRS information is more transparent than in EPSG, but can be more difficult to read and compare than PROJ since it is meant to necessarily represent more complex CRS information. Additionally, the WKT standard is implemented inconsistently across various software platforms, and the spec itself has some known issues.\n\n\n4.6.3 PROJ\nPROJ is an open-source library for storing, representing and transforming CRS information. PROJ strings continue to be used, but the format is deprecated by the PROJ C maintainers due to inaccuracies when converting to the WKT format. The data and Python libraries we will be working with in this workshop use different underlying representations of CRSs under the hood for reprojecting. CRS information can still be represented with EPSG, WKT, or PROJ strings without consequence, but it is best to only use PROJ strings as a format for viewing CRS information, not for reprojecting data.\nPROJ represents CRS information as a text string of key-value pairs, which makes it easy to read and interpret.\nA PROJ4 string includes the following information:\n\nproj: the projection of the data\nzone: the zone of the data (this is specific to the UTM projection)\ndatum: the datum used\nunits: the units for the coordinates of the data\nellps: the ellipsoid (how the earth’s roundness is calculated) for the data\n\nNote that the zone is unique to the UTM projection. Not all CRSs will have a zone.\n\n\n\nThe UTM zones across the continental United States (Chrismurf at English Wikipedia, via Wikimedia Commons (CC-BY))"
  },
  {
    "objectID": "03_intro_crs.html#format-interoperability",
    "href": "03_intro_crs.html#format-interoperability",
    "title": "4  Introduction to coordinate reference systems",
    "section": "4.7 Format interoperability",
    "text": "4.7 Format interoperability\nMany existing file formats were invented by GIS software developers, often in a closed-source environment. This led to the large number of formats on offer today, and considerable problems transferring data between software environments. The Geospatial Data Abstraction Library (GDAL) is an open-source answer to this issue. In addition to there being multiple file formats, there are also many different projected coordinate reference systems in use and we often need to reproject data to make it consistent with the CRS of some other data set. PROJ is the answer to this problem.\n\n4.7.1 GDAL\nGDAL is an indispensable part of computational geospatial work. What is it?\n\na translator library for raster and vector geospatial data formats (a few hundred) written in C, C++ and Python,\nan open source package (MIT License) released by The Open Source Geospatial Foundation (OSGeo),\nin addition to being used as a callable library, it includes a set of command line tools,\nis used as a core resource in countless GIS and geospatial analysis tools (e.g., free and open-source packages such as QGIS and GRASS; even ESRI appears to use GDAL to deal with custom raster formats).\n\nGDAL is a set of software tools that translate between almost any geospatial format in common use today (and some not so common ones). GDAL also contains tools for editing and manipulating both raster and vector files, including reprojecting data to different CRSs. GDAL can be used as a standalone command-line tool, or built in to other GIS software. Several open-source GIS programs use GDAL for all file import/export operations.\nAnother related library, OGR, is part of the GDAL source code and focuses on “simple features vector data”. This GDAL FAQ page gives more detail on the GDAL/OGR relationship. When people say GDAL, it includes OGR. Speaking of saying, both “gee-doll” and “goo-dle” are used.\nGDAL/OGR also relies on the PROJ library for projections and transformations.\nGiven the importance of GDAL and its use by so many geospatial software packages, it is somewhat surprising that for many years it was maintained by a single person. Check out this Mapscaping podcast on GDAL for a fascinating telling of the GDAL story.\nWe usually don’t have to install GDAL as it will get installed when we install higher level packages such as GeoPandas.\n\n\n4.7.2 PROJ - transform geospatial coordinates between different coordinate reference systems\nThe PROJ library does the heavy lifting of translating between different CRS and projections. Much like GDAL, it is:\n\nopen source,\nused both as a library and command line tool,\nfoundational software to geospatial analysis,\nnow maintained by OSGeo.\n\nWhile the underlying library is C/C++, you can use PROJ from Python via the pyproj package. Like GDAL, pyproj will get installed when we install a higher level package such as GeoPandas."
  },
  {
    "objectID": "03_intro_crs.html#metadata",
    "href": "03_intro_crs.html#metadata",
    "title": "4  Introduction to coordinate reference systems",
    "section": "4.8 Metadata",
    "text": "4.8 Metadata\nSpatial data is useless without metadata. Essential metadata includes the CRS information, but proper spatial metadata encompasses more than that. History and provenance of a dataset (how it was made), who is in charge of maintaining it, and appropriate (and inappropriate!) use cases should also be documented in metadata. This information should accompany a spatial dataset wherever it goes. In practice this can be difficult, as many spatial data formats don’t have a built-in place to hold this kind of information. Metadata often has to be stored in a companion file, and generated and maintained manually."
  },
  {
    "objectID": "03_intro_crs.html#more-resources-on-crs",
    "href": "03_intro_crs.html#more-resources-on-crs",
    "title": "4  Introduction to coordinate reference systems",
    "section": "4.9 More Resources on CRS",
    "text": "4.9 More Resources on CRS\n\nspatialreference.org - A comprehensive online library of CRS information.\nQGIS Documentation - CRS Overview.\nChoosing the Right Map Projection.\nVideo highlighting how map projections can make continents seems proportionally larger or smaller than they actually are.\n\n\n\n\n\n\n\nCRS and projections\n\n\n\nIf you are going to work with geospatial data, you are going to have to learn about coordinate reference systems (CRS) and map projections. The world isn’t flat and it’s not a perfect sphere. However, most maps are flat. Projections are a way of translating our non-flat earth to a flat representation for mapping. The CRS is a specific type of “grid system” so that numeric X-Y coordinates can be associated with any point on the map.\n\nAll geospatial datasets (raster and vector) are associated with a specific coordinate reference system.\nA coordinate reference system includes datum, projection, and additional parameters specific to the dataset.\n\n\n\nA few interesting reference documents on CRS use by the USGS include:\n\nhttps://pubs.usgs.gov/pp/1395/report.pdf\nhttps://www.usgs.gov/publications/map-projections-used-us-geological-survey"
  },
  {
    "objectID": "03_intro_crs.html#case-study-land-use-analysis-on-the-ou-campus",
    "href": "03_intro_crs.html#case-study-land-use-analysis-on-the-ou-campus",
    "title": "4  Introduction to coordinate reference systems",
    "section": "4.10 Case Study: Land use analysis on the OU campus",
    "text": "4.10 Case Study: Land use analysis on the OU campus\nFor this this module, you’ll be working through a Jupyter notebook that introduces the very basics of working with coordinate reference systems.\n\n4.10.1 Activities\nLaunch Jupyter lab and open the ou_land_use_03_crs.ipynb file. Work your way through out."
  },
  {
    "objectID": "04_gis_landscape.html#standalone-software-packages",
    "href": "04_gis_landscape.html#standalone-software-packages",
    "title": "5  The GIS landscape",
    "section": "5.1 Standalone software packages",
    "text": "5.1 Standalone software packages\nMost traditional GIS work is carried out in standalone applications that aim to provide end-to-end geospatial solutions. These applications are available under a wide range of licenses and price points. Some of the most common are listed below.\n\n5.1.1 Open-source software\nThe Open Source Geospatial Foundation (OSGEO) supports several actively managed GIS platforms:\n\nQGIS is a professional GIS application that is built on top of and proud to be itself Free and Open Source Software (FOSS). QGIS is written in Python, has a python console interface, and has several interfaces written in R including RQGIS.\nGRASS GIS, commonly referred to as GRASS (Geographic Resources Analysis Support System), is a FOSS-GIS software suite used for geospatial data management and analysis, image processing, graphics and maps production, spatial modeling, and visualization. GRASS GIS is currently used in academic and commercial settings around the world, as well as by many governmental agencies and environmental consulting companies. It is a founding member of the Open Source Geospatial Foundation (OSGeo). GRASS GIS can be installed along with and made accessible within QGIS 3.\nGDAL is a multiplatform set of tools for translating between geospatial data formats. It can also handle reprojection and a variety of geoprocessing tasks. GDAL is built in to many applications both FOSS and commercial, including GRASS and QGIS.\nSAGA-GIS, or System for Automated Geoscientific Analyses, is a FOSS-GIS application developed by a small team of researchers from the Dept. of Physical Geography, Göttingen, and the Dept. of Physical Geography, Hamburg. SAGA has been designed for an easy and effective implementation of spatial algorithms, offers a comprehensive, growing set of geoscientific methods, provides an easily approachable user interface with many visualisation options, and runs under Windows and Linux operating systems. Like GRASS GIS, it can also be installed and made accessible in QGIS3.\nPostGIS is a geospatial extension to the PostGreSQL relational database.\n\n\n\n5.1.2 Commercial software\n\nESRI (Environmental Systems Research Institute) is an international supplier of geographic information system (GIS) software, web GIS and geodatabase management applications. ESRI provides several licenced platforms for performing GIS, including ArcGIS, ArcGIS Online, and Portal for ArcGIS a standalone version of ArGIS Online which you host locally. ESRI welcomes development on their platforms through their DevLabs. ArcGIS software can be installed using Chef Cookbooks from Github.\nPitney Bowes produced MapInfo Professional, which was one of the earliest desktop GIS programs on the market.\nHexagon Geospatial Power Portfolio includes many geospatial tools including ERDAS Imagine, powerful software for remote sensing.\nManifold is a desktop GIS that emphasizes speed through the use of parallel and GPU processing.\n\n\n\n5.1.3 Online + cloud computing\n\nPANGEO is a community organization dedicated to open and reproducible data science with python. They focus on the Pangeo software ecosystem for working with big data in the geosciences.\nGoogle has created Google Earth Engine (GEE) which combines a multi-petabyte catalog of satellite imagery and geospatial datasets with planetary-scale analysis capabilities and makes it available for scientists, researchers, and developers to detect changes, map trends, and quantify differences on the Earth’s surface. Earth Engine API runs in both Python and JavaScript.\nMicrosoft has created the Planetary Computer which is similar to GEE but focuses on open standards such as the STAC catalog for organizing and finding geospatial data. It is heavily Python focused and its compute takes place in a JupyterHub. Both GEE and the Planetary Computer are free for academic and personal use.\nArcGIS Online provides access to thousands of maps and base layers.\n\nYou can find a comparison of GEE and the Planetary Computer at the Mapscaping blog.\nPrivate companies have released SDK platforms for large scale GIS analysis:\n\nKepler.gl is Uber’s toolkit for handling large datasets (i.e. Uber’s data archive).\nBoundless Geospatial is built upon OSGEO software for enterprise solutions.\n\nPublicly funded open-source platforms for large scale GIS analysis:\n\nPANGEO for the Earth Sciences. This community organization also supports python libraries like xarray, iris, dask, jupyter, and many other packages.\nSepal.io by FAO Open Foris utilizing EOS satellite imagery and cloud resources for global forest monitoring."
  },
  {
    "objectID": "04_gis_landscape.html#gui-vs-cli",
    "href": "04_gis_landscape.html#gui-vs-cli",
    "title": "5  The GIS landscape",
    "section": "5.2 GUI vs CLI",
    "text": "5.2 GUI vs CLI\nThe earliest computer systems operated without a graphical user interface (GUI), relying only on the command-line interface (CLI). Since mapping and spatial analysis are strongly visual tasks, GIS applications benefited greatly from the emergence of GUIs and quickly came to rely heavily on them. Most modern GIS applications have very complex GUIs, with all common tools and procedures accessed via buttons and menus.\nBenefits of using a GUI include:\n\nTools are all laid out in front of you\nComplex commands are easy to build\nDon’t need to learn a coding language\nCartography and visualisation is more intuitive and flexible\n\nDownsides of using a GUI include:\n\nLow reproducibility - you can’t record your actions and replay\nMost are not designed for batch-processing files\nLimited ability to customise functions or write your own\nIntimidating interface for new users - so many buttons!\n\nIn scientific computing, the lack of reproducibility in point-and-click software has come to be viewed as a critical weakness. As such, scripted CLI-style workflows are again becoming popular, which leads us to another approach to doing GIS — via a programming language. This is the approach we will be using throughout this module."
  },
  {
    "objectID": "04_gis_landscape.html#gis-in-programming-languages",
    "href": "04_gis_landscape.html#gis-in-programming-languages",
    "title": "5  The GIS landscape",
    "section": "5.3 GIS in programming languages",
    "text": "5.3 GIS in programming languages\nA number of powerful geospatial processing libraries exist for general-purpose programming languages like Java and C++. However, the learning curve for these languages is steep and the effort required is excessive for users who only need a subset of their functionality.\nHigher-level scripting languages like Python and R are easier to learn and use. Both now have their own packages that wrap up those geospatial processing libraries and make them easy to access and use safely. A key example is the Java Topology Suite (JTS), which is implemented in C++ as GEOS. GEOS is accessible in Python via the shapely package (and geopandas, which makes use of shapely) and in R via sf. R and Python also have interface packages for GDAL, and for specific GIS apps.\nThis last point is a huge advantage for GIS-by-programming; these interface packages give you the ability to access functions unique to particular programs, but have your entire workflow recorded in a central document - a document that can be re-run at will. Below are lists of some of the key spatial packages for Python.\n\ngeopandas and geocube for working with vector data\nrasterio and rioxarray for working with raster data\n\nThese packages along with the matplotlib package are all we need for spatial data visualisation. Python also has many fundamental scientific packages that are relevant in the geospatial domain. Below is a list of particularly fundamental packages. numpy, scipy, and scikit-image are all excellent options for working with rasters, as arrays.\nAs a programming language, Python can be a CLI tool. However, using Python together with an Integrated Development Environment (IDE) application allows some GUI features to become part of your workflow. IDEs allow the best of both worlds. They provide a place to visually examine data and other software objects, interact with your file system, and draw plots and maps, but your activities are still command-driven: recordable and reproducible. There are several IDEs available for Python. JupyterLab is well-developed and the most widely used option for data science in Python. VSCode and Spyder are other popular options for data science.\nTraditional GIS apps are also moving back towards providing a scripting environment for users, further blurring the CLI/GUI divide. ESRI have adopted Python into their software, and QGIS is both Python and R-friendly."
  },
  {
    "objectID": "04_gis_landscape.html#gis-file-types",
    "href": "04_gis_landscape.html#gis-file-types",
    "title": "5  The GIS landscape",
    "section": "5.4 GIS file types",
    "text": "5.4 GIS file types\nThere are a variety of file types that are used in GIS analysis. Depending on the program you choose to use some file types can be used while others are not readable. Below is a brief table describing some of the most common vector and raster file types.\nSee https://en.wikipedia.org/wiki/GIS_file_format for more about GIS file formats.\n\n5.4.1 Vector\n\n\n\n\n\n\n\n\nFile Type\nExtensions\nDescription\n\n\n\n\nEsri Shapefile\n.SHP .DBF .SHX\nThe most common geospatial file type. This has become the industry standard. The three required files are: SHP is the feature geometry. SHX is the shape index position. DBF is the attribute data.\n\n\nGeographic JavaScript Object Notation (GeoJSON)\n.GEOJSON .JSON\nUsed for web-based mapping and uses JavaScript Object Notation to store the coordinates as text.\n\n\nGoogle Keyhole Markup Language (KML)\n.KML .KMZ\nKML stands for Keyhole Markup Language. This GIS format is XML-based and is primarily used for Google Earth.\n\n\nOpenStreetMap\n.OSM\nOSM files are the native file for OpenStreetMap which had become the largest crowdsourcing GIS data project in the world. These files are a collection of vector features from crowd-sourced contributions from the open community.\n\n\n\n\n\n5.4.2 Raster\n\n\n\n\n\n\n\n\nFile Type\nExtensions\nDescription\n\n\n\n\nERDAS Imagine\n.IMG\nERDAS Imagine IMG files is a proprietary file format developed by Hexagon Geospatial. IMG files are commonly used for raster data to store single and multiple bands of satellite data. Each raster layer as part of an IMG file contains information about its data values. For example, this includes projection, statistics, attributes, pyramids and whether or not it’s a continuous or discrete type of raster.\n\n\nGeoTIFF\n.TIF .TIFF .OVR\nThe GeoTIFF has become an industry image standard file for GIS and satellite remote sensing applications. GeoTIFFs may be accompanied by other files:TFW is the world file that is required to give your raster geolocation.XML optionally accompany GeoTIFFs and are your metadata.AUX auxiliary files store projections and other information.OVR pyramid files improves performance for raster display.\n\n\nCloud Optimized GeoTIFF (COG)\n.TIF .TIFF\nBased on the GeoTIFF standard, COGs incorporate tiling and overviews to support HTTP range requests where users can query and load subsets of the image without having to transfer the entire file.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nMany software packages exist for working with geospatial data.\nCommand-line programs allow you to automate and reproduce your work.\nJupyterLab provides a user-friendly interface for working with Python."
  },
  {
    "objectID": "06_working_raster_data.html#intro-and-objectives",
    "href": "06_working_raster_data.html#intro-and-objectives",
    "title": "7  More on Working with raster and vector data",
    "section": "7.1 Intro and Objectives",
    "text": "7.1 Intro and Objectives\nIt is quite common that the raster data you have in hand is too large to process, or not all the pixels are relevant to your area of interest (AoI). In both situations, you should consider cropping your raster data before performing data analysis.\nIn this module, we will introduce how to crop raster data into the desired area. We will use one Sentinel-2 image as the example raster data, and introduce how to crop your data to different types of AoIs.\nOur objective are:\n\nRaster data computations\nWorking with nodata values\nClipping or cropping of raster data"
  },
  {
    "objectID": "06_working_raster_data.html#readings-and-resources",
    "href": "06_working_raster_data.html#readings-and-resources",
    "title": "7  More on working with raster and vector data",
    "section": "7.1 Readings and Resources",
    "text": "7.1 Readings and Resources\n\nGCwP - Section 3.3: Spatial operations on raster data\nGCwP - Section 4.3: Geometric operations on raster data"
  },
  {
    "objectID": "06_working_raster_data.html#introduce-the-data",
    "href": "06_working_raster_data.html#introduce-the-data",
    "title": "7  More on working with raster and vector data",
    "section": "7.2 Introduce the data",
    "text": "7.2 Introduce the data\nWe will use the image file that we downloaded and saved from the last notebook. You can find the T17TLH_20240401T162831_TCI_10m.tif file in data folder in the compressed file you downloaded in the first section of this module.\n\n7.2.1 Activities\nLaunch Jupyter lab and open the ou_land_use_06_raster_work.ipynb file. Work your way through it."
  },
  {
    "objectID": "05_access_satellite_imagery.html#intro-and-objectives",
    "href": "05_access_satellite_imagery.html#intro-and-objectives",
    "title": "6  Access satellite imagery using Python",
    "section": "6.1 Intro and Objectives",
    "text": "6.1 Intro and Objectives\nA number of satellites take snapshots of the Earth’s surface from space. The images recorded by these remote sensors represent a very precious data source for any activity that involves monitoring changes on Earth. Satellite imagery is typically provided in the form of geospatial raster data, with the measurements in each grid cell (“pixel”) being linked with accurate geographic coordinate information.\nIn this episode we will explore how to access open satellite data using Python. In particular, we will consider the Sentinel-2 data collection that is hosted on the Microsoft Planetary Computer. This dataset consists of multi-band optical images acquired by the two satellites of the Sentinel-2 mission and it is continuously updated with new images. After reading about the Sentinel-2 mission, you might have a number of questions:\n\nHow do we do basic things like find images (geoTIFF) files and save the image itself to disk?\nWhat are these STAC repositories?\nWhat exactly is in this imaging data? There are numerous bands with different types of data and at different resolution levels (size in meters of each cell width).\nHow do we read this imaging data into data structures amenable to further analysis?\nWhat kinds of analysis can we do with this data?\nWhat existing resources are available for newbies to Sentinel image data analysis?\n\nIn this section we learn to do the following:\n\nSearch public STAC repositories of satellite imagery using Python.\nInspect search result’s metadata.\nDownload (a subset of) the assets available for a satellite scene.\nOpen satellite imagery as raster data and save it to disk."
  },
  {
    "objectID": "05_access_satellite_imagery.html#overview-of-the-planetary-computer",
    "href": "05_access_satellite_imagery.html#overview-of-the-planetary-computer",
    "title": "6  Access satellite imagery using Python",
    "section": "6.1 Overview of the Planetary Computer",
    "text": "6.1 Overview of the Planetary Computer\nMicrosoft’s, pretty new, Planetary Computer is quite an ambitious project. It has several major components:\n\nData Catalog - It is a ginormous repository of well cataloged data all about Earth’s various systems and includes a web based interface that allows users to find relevant data - for free. The repository is based on an open standard known as STAC.\nAPI - It has an API that leverages open source tools to make it easy to do data searches by time and location. Focuses on Python.\nHub - A managed compute environment for doing cloud based geospatial analysis at scale. For this part you need to apply for access.\nApplications - An ecosystem of people doing meaningful work with the Planetary Computer.\n\nThere is a Mapscaping podcast featuring two of the main developers of the Planetary Computer - see https://mapscaping.com/podcast/the-planetary-computer/. The Mapscaping.com site is a great place to learn about geospatial analysis through its terrific podcasts. For example:\n\nThe Planetary Computer - https://mapscaping.com/podcast/the-planetary-computer/\nIntroducing Google Earth Engine - https://mapscaping.com/podcast/introducing-google-earth-engine/\nSentinel Hub - https://mapscaping.com/podcast/sentinel-hub/\n\nWe’ll mostly be using the Planetary Computer to search for and acquire satellite image data from the Sentinel-2 and Landsat catalogs. We can use the PySTAC package to do this freely and without any authentication. As mentioned above, authentication is required to use the Hub computing cluster."
  },
  {
    "objectID": "05_access_satellite_imagery.html#search-for-satellite-imagery",
    "href": "05_access_satellite_imagery.html#search-for-satellite-imagery",
    "title": "6  Access satellite imagery using Python",
    "section": "6.2 Search for satellite imagery",
    "text": "6.2 Search for satellite imagery\n\n6.2.1 The SpatioTemporal Asset Catalog (STAC) specification\nCurrent sensor resolutions and satellite revisit periods are such that terabytes of data products are added daily to the corresponding collections. Such datasets cannot be made accessible to users via full-catalog download. Space agencies and other data providers often offer access to their data catalogs through interactive portals such as the Copernicus Open Access Hub portal for the Sentinel missions. Accessing data via a GUI is a nice way to explore a catalog and get familiar with its content, but it represents a heavy and error-prone task that should be avoided if carried out systematically to retrieve data.\nA service that offers programmatic access to the data enables users to access the desired data in a more reliable, scalable and reproducible manner. An important element in the software interface exposed to the users, which is generally called the Application Programming Interface (API), is the use of standards. Standards, in fact, can significantly facilitate the reusability of tools and scripts across datasets and applications.\nThe SpatioTemporal Asset Catalog (STAC) specification is an emerging standard for describing geospatial data. By organizing metadata in a form that adheres to the STAC specifications, data providers make it possible for users to access data from different missions, instruments and collections using the same set of tools."
  },
  {
    "objectID": "05_access_satellite_imagery.html#search-a-stac-catalog",
    "href": "05_access_satellite_imagery.html#search-a-stac-catalog",
    "title": "6  Access satellite imagery using Python",
    "section": "6.3 Search a STAC catalog",
    "text": "6.3 Search a STAC catalog\nThere are general STAC browsers such as the one at Radiant Earth. It is good starting point to discover available datasets, as it provides an up-to-date list of existing STAC catalogs. However, we are going to use the STAC browsing tools available through the Planetary Computer."
  },
  {
    "objectID": "05_access_satellite_imagery.html#exercise-discover-a-stac-catalog",
    "href": "05_access_satellite_imagery.html#exercise-discover-a-stac-catalog",
    "title": "6  Access satellite imagery using Python",
    "section": "6.4 Exercise: Discover a STAC catalog",
    "text": "6.4 Exercise: Discover a STAC catalog\nLet’s take a moment to explore the Planetary Computer STAC catalog, which contains a large number of datasets spanning numerous areas such as air quality, climate, biodiversity, imagery, fires, weather, land use and more. Start by going to the main Data Catalog page. Spend some time browsing the various categories to see the breadth of data that is available. Then go to the Sentinel-2 Level-2A page. Click the big blue button on the right labelled View in Explorer. You can navigate around somewhat like you do with Google Earth or Maps. Find the OU campus. As you scroll around on the map, the list of image tiles will continually update along the left side of the screen. Notice that the default is the most recent images with low cloud cover at the top.\nPick one of the images from the list on the left. If you click, the Show on Map icon in the lower right corner of the image preview, you’ll see the satellite image superimposed on the map. If you click the squiggly bracket icon just to the left of the Show on Map icon, you’ll get a window with a Python code snippet for accessing the assets of this map. We’ll learn more about this shortly.\n\n\n\nViews of the STAC browser\n\n\n\n\n\n\n\n\nChallenge - Extent\n\n\n\nScroll down through the image properties on the left. What CRS is this image using?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe EPSG code is 32627. This is associated with the CRS WGS 84 / UTM zone 17N. See https://epsg.io/?q=32617."
  },
  {
    "objectID": "05_access_satellite_imagery.html#cloud-optimized-geotiffs",
    "href": "05_access_satellite_imagery.html#cloud-optimized-geotiffs",
    "title": "6  Access satellite imagery using Python",
    "section": "6.5 Cloud Optimized GeoTIFFs",
    "text": "6.5 Cloud Optimized GeoTIFFs\nCloud Optimized GeoTIFFs (COGs) are regular GeoTIFF files with some additional features that make them ideal to be employed in the context of cloud computing and other web-based services. This format builds on the widely-employed GeoTIFF format, already introduced in Episode 1: Introduction to Raster Data. In essence, COGs are regular GeoTIFF files with a special internal structure. One of the features of COGs is that data is organized in “blocks” that can be accessed remotely via independent HTTP requests. Data users can thus access the only blocks of a GeoTIFF that are relevant for their analysis, without having to download the full file. In addition, COGs typically include multiple lower-resolution versions of the original image, called “overviews”, which can also be accessed independently. By providing this “pyramidal” structure, users that are not interested in the details provided by a high-resolution raster can directly access the lower-resolution versions of the same image, significantly saving on the downloading time. More information on the COG format can be found here.\n\n6.5.1 Activities\nLaunch Jupyter lab and open the ou_land_use_05_satellite.ipynb file. Work your way through it."
  },
  {
    "objectID": "02_intro_vector_data.html#important-attributes-of-vector-data",
    "href": "02_intro_vector_data.html#important-attributes-of-vector-data",
    "title": "3  Introduction to vector data",
    "section": "3.3 Important attributes of vector data",
    "text": "3.3 Important attributes of vector data\n\n3.3.1 Extent\nThe spatial extent is the geographic area that the geographic data covers. The spatial extent of an object represents the geographic edge or location that is the furthest north, south, east and west. In other words, extent represents the overall geographic coverage of the spatial object.\n\n\n\nSpatial extent image (Image Source: National Ecological Observatory Network (NEON))\n\n\n\n\n\n\n\n\nChallenge - Extent\n\n\n\nIn the image above, the dashed boxes around each set of objects seems to imply that the three objects have the same extent. Is this accurate? If not, which object(s) have a different extent?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe lines and polygon objects have the same extent. The extent for the points object is smaller in the vertical direction than the other two because there are no points on the line at y = 8."
  }
]